{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed set of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "from datetime import timedelta\n",
    "import hampel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import rednt_tools as tools  # custom helper tools library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV files with timestamp as index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df = pd.read_csv(tools.VIBRATION_FILENAME, sep=tools.SEP, header=None,\n",
    "                   names=tools.V_HEADER_NAMES, index_col=[0], parse_dates=[0],\n",
    "                   date_parser=tools.date_parser)\n",
    "\n",
    "t_df = pd.read_csv(tools.TEMPERATURE_FILENAME, sep=tools.SEP, header=None,\n",
    "                   names=tools.T_HEADER_NAMES, index_col=[0], parse_dates=[0],\n",
    "                   date_parser=tools.date_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that both of the data are sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df = v_df.sort_values(by='timestamp')\n",
    "t_df = t_df.sort_values(by='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine some timestamp characterstics - start time, end time and time horizon of signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_chars = {\n",
    "    'vibration': {\n",
    "        'start': v_df.index.to_series()[0],\n",
    "        'end': v_df.index.to_series()[-1],\n",
    "        'horizon': v_df.index.to_series()[-1] - v_df.index.to_series()[0]\n",
    "    },\n",
    "    'temperature': {\n",
    "        'start': t_df.index.to_series()[0],\n",
    "        'end': t_df.index.to_series()[-1],\n",
    "        'horizon': t_df.index.to_series()[-1] - v_df.index.to_series()[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Timestamp characteristics:\")\n",
    "print(timestamp_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate signal timestamp differences between samples and some statistics about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_timestamp_diff = v_df.index.to_series().diff()\n",
    "t_timestamp_diff = t_df.index.to_series().diff()\n",
    "\n",
    "timestamp_diff = pd.concat([v_timestamp_diff, t_timestamp_diff])\n",
    "timedelta_value_counts = timestamp_diff.value_counts()\n",
    "\n",
    "print('Timestamp differences:')\n",
    "print(timedelta_value_counts)\n",
    "\n",
    "timedelta_statistics = {\n",
    "    'mean': timestamp_diff.mean(),\n",
    "    'median': timestamp_diff.median(),\n",
    "    'mode': timestamp_diff.mode(),\n",
    "    'max': timestamp_diff.max(),\n",
    "    'min': timestamp_diff.min()\n",
    "}\n",
    "\n",
    "print('Timestamp differences statistics:')\n",
    "print(timedelta_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduce sampling time with timedelta as the median from the diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_time = timedelta_statistics['median']\n",
    "\n",
    "print(\"Median Timedelta that we choose as sampling time: {}\".format(sampling_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample the data to use deduced sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df = v_df.resample(sampling_time).mean()\n",
    "t_df = t_df.resample(sampling_time).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers using hampel filter with remembering indices of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_HAMPEL = False\n",
    "\n",
    "if USE_HAMPEL:\n",
    "    hampel_args = {\n",
    "        'window_size': 50,\n",
    "        'n': 3\n",
    "    }\n",
    "\n",
    "    v_outliers = hampel.hampel(v_df.squeeze(), **hampel_args)\n",
    "    t_outliers = hampel.hampel(t_df.squeeze(), **hampel_args)\n",
    "    print(\"Number of outliers in data: vibration: \"\n",
    "        \"{}%, temperature: {}%\".format(len(v_outliers)*100/v_df.shape[0],\n",
    "                                        len(t_outliers)*100/t_df.shape[0]))\n",
    "    v_df[v_df.columns[0]] = hampel.hampel(v_df.squeeze(), **hampel_args, imputation=True)\n",
    "    t_df[t_df.columns[0]] = hampel.hampel(t_df.squeeze(), **hampel_args, imputation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_INTERPOLATION = True\n",
    "\n",
    "if USE_INTERPOLATION:\n",
    "    interp_args = {\n",
    "        'method': 'linear',\n",
    "        'limit': int(timedelta(minutes=1) / sampling_time),\n",
    "        'limit_area': 'inside'\n",
    "    }\n",
    "    v_df = v_df.interpolate(**interp_args)\n",
    "    t_df = t_df.interpolate(**interp_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataframes based on the closest timestamp and limit to smallest length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tolerance = sampling_time\n",
    "if len(v_df) > len(t_df):\n",
    "    df = pd.merge_asof(t_df, v_df, on='timestamp', tolerance=merge_tolerance)\n",
    "else:\n",
    "    df = pd.merge_asof(v_df, t_df, on='timestamp', tolerance=merge_tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set index after merge operation to timestamp again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PLOT_WHOLE_DF = False\n",
    "\n",
    "if USE_PLOT_WHOLE_DF:\n",
    "    tools.plot_timeseries_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into 1-hour length time horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups = df.groupby([(df.index - df.index[0]).astype('timedelta64[1h]')])\n",
    "groups_num = len(df_groups)\n",
    "\n",
    "print(\"Number of groups: {}\".format(groups_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate group timeseries to fill out some missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_names = [15, 1024, 1738]\n",
    "\n",
    "for name, group in df_groups:\n",
    "    if name in verbose_names:\n",
    "        print(name)\n",
    "        tools.plot_timeseries_data(group, style=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the features for each group (ref. https://www.jvejournals.com/article/21622)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDITY_THRESHOLD = 0.4\n",
    "\n",
    "non_valid_counter = 0\n",
    "\n",
    "names_list = list()\n",
    "features_list = list()\n",
    "for name, group in df_groups:\n",
    "    # Calculate number of NaNs to check validity and igniore group\n",
    "    nans_count = group.isna().sum().sum()\n",
    "    invalid_part = (nans_count / (group.shape[0]*group.shape[1]))\n",
    "    # Skip feature extraction when the percentage of invalid values is too high\n",
    "    if invalid_part > VALIDITY_THRESHOLD:\n",
    "        non_valid_counter += 1\n",
    "        if name in verbose_names:\n",
    "            print(\"Group {} ignored on validity check.\".format(name))\n",
    "        continue\n",
    "\n",
    "    # Drop NaNs in the vibration data\n",
    "    group.dropna(subset=['vibration'], inplace=True)\n",
    "\n",
    "    # Calculate velocity signal\n",
    "    velocity = integrate.cumulative_trapezoid(group['vibration'],\n",
    "                                              x=group.index.astype(np.int64) / 10**9)\n",
    "    # Extract features\n",
    "    features = {\n",
    "        'temperature': group['temperature'].mean(),\n",
    "        'acc_rms': np.sqrt(group['vibration'].pow(2).mean()),\n",
    "        'acc_kurtosis': group['vibration'].kurt(),\n",
    "        'acc_peak_to_peak': group['vibration'].max() - group['vibration'].min(),\n",
    "        'vel_rms': np.sqrt(np.square(velocity).mean())\n",
    "    }\n",
    "    names_list.append(name)\n",
    "    features_list.append(features)\n",
    "    if name in verbose_names:\n",
    "        print(\"Group {}: {}\".format(name, features))\n",
    "\n",
    "print(\"Ignored {} groups due to too many missing values.\".format(non_valid_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrame from features list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(data=features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate features importance based on normalized variance (versus mean values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = (df_features.var() / df_features.mean()).sort_values(ascending=False)\n",
    "print(\"Feature importance:\\n{}\".format(feature_importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.plot.scatter(x=feature_importance.index[3], y=feature_importance.index[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the data using DBSCAN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc790433a874869e0e619865f0d14cdf0639ffcd471a3c2d17dd0c72c1821ae4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('rednt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
